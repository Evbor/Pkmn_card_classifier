{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pokemon Card Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as k\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import backend as K\n",
    "import base_set_classifier.data as data\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "K.set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def print_experiments(mnt_pt):\n",
    "    for experiment in os.listdir(mnt_pt):\n",
    "        experiment_path = mnt_pt + '/' + experiment\n",
    "        print('Experiment {0} directory: {1}'.format(experiment, experiment_path))\n",
    "        for directory in os.listdir(experiment_path):\n",
    "            if directory == 'dataset':\n",
    "                print('dataset path: {} \\n'.format(experiment_path + '/' + directory))\n",
    "            else:\n",
    "                print('no dataset directory in experiment directory: {0}, subdirectories are: {1}'.format(experiment, os.listdir(experiment_path)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Constants\n",
    "# file constants\n",
    "data_csv_path = '/media/Data/Programs/Pkmn_card_classifier/data/processed.csv'\n",
    "images_path = '/media/Data/Programs/Pkmn_card_classifier/data/images/'\n",
    "# data properties\n",
    "img_size = (550,400) # (height, width) in pixels\n",
    "color_mode = 'rgb'\n",
    "batch_size = 32\n",
    "random_seed = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10984 entries, 0 to 10983\n",
      "Data columns (total 2 columns):\n",
      "filename    10984 non-null object\n",
      "class       10984 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 171.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(data_csv_path)\n",
    "dataset_size = len(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Frequency Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f30d4097358>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_count = df['class'].value_counts()\n",
    "class_count.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not_base_set    10882\n",
      "base_set          102\n",
      "Name: class, dtype: int64\n",
      "\n",
      "Expected value for probability of drawing a not_base_set sample from our dataset: 0.9907137654770576\n",
      "Expected value for probability of drawing a base_set sample from our dataset: 0.009286234522942461\n"
     ]
    }
   ],
   "source": [
    "print(class_count)\n",
    "print()\n",
    "for c in class_count.index:\n",
    "    print('Expected value for probability of drawing a {0} sample from our dataset: {1}'.format(c, class_count[c] / dataset_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see from the above histogram that our dataset is very unbalanced and that we can expect an average (expected value) accuracy of 0.9907 for classification if we constructed a model that only predicted not_base_set for all examples in our sample dataset since the probability of encountering a non_base_set example is 0.9907. A classifier of this design would have great accuracy scores but would have trouble predicting examples that are truely base_set. This suggests that classification accuracy might not be the best measure of our models performance and that maybe we should look at the precision, recall, and F1 scores of our specific classes. Assuming our dataset sample is a representative sample of the total population of all possible base set pokemon card images and non base set pokemon card images that will ever be uploaded to the app then there would be no issue with the unbalancedness of this dataset, but if it isn't a representative sample of the population then our classifier would have a heavy bias on predicting not base set. This assumption is unlikely to be valid because there is no good reason to suggest that users would prefer to upload more images of one class than the other. Which means that a classifier trained on this dataset and cross validated for highest accuracy would most likely be heavily biased on labeling examples as non base set, and would perform pretty poorly when deployed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of ways we can deal with the above problem.\n",
    "1. Undersample our most represented class, and maximize accuracy when we think our sample is representative. The issue though is the amount of examples of our under represented class (or over represented class after undersampling) might not be enough for our model to learn to identify them, and we would have to bias our model more in order to achieve better results (but overall worse results than our higher variance model trained on more data)\n",
    "2. Oversample our under represented class, and maximize accuracy when we think our sample is representative. The issue with this solution is that at some point if we oversample too much the model might overtrain on the under represented class and not generalize well in deployment.\n",
    "3. Oversample our under represented class with image augmentation. Again the downside to this solution is that our image augmentation might not be strong enough to produce examples that are different enough from the ones we started with and this will eventually cause overtraining.\n",
    "4. Brute force search for model architectures that can learn to maximize our precision, recall, and F1 metrics from the limited amount of data we have for our under represented class. The problem with this solution is that the models we might find might not perform as well as we want on these metrics because the models that are capable of learning from our limited amount of data for our under represented class might be heavily biased, making the overall scores on our metrics not high.\n",
    "5. Change our loss function to penalize wrong classification of the under represented class more than that of the over represented class.\n",
    "6. Gather more data!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are more concerned with our model identifying base set examples than non base set examples the metrics we should be concerned with are the precision, recall and F1 scores for our base set class. **Since recall is essentially our accuracy score on predicting base set examples, and precision is our measure of certainty on our predictions of base set, we naturally would want to maximize both. This implies we should maximize our F1 score since it is the unweighted harmonic mean of our precision and recall scores.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions 1-3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "\n",
    "def print_dataset_info(df, name):\n",
    "    \"\"\"\n",
    "    Prints size of dataframe, and value counts\n",
    "    df: type DataFrame, with columns=['filename', 'class']\n",
    "    name: type String, name of the dataset\n",
    "    ---> None\n",
    "    \"\"\"\n",
    "    df_size = len(df)\n",
    "    class_count = df['class'].value_counts()\n",
    "    print('Size of {0}: {1}'.format(name, df_size))\n",
    "    print()\n",
    "    print('Class counts for {0}:'.format(name))\n",
    "    print(class_count)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 6590\n",
      "\n",
      "Class counts for training set:\n",
      "not_base_set    6529\n",
      "base_set          61\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "Size of validation set: 2197\n",
      "\n",
      "Class counts for validation set:\n",
      "not_base_set    2176\n",
      "base_set          21\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "Size of test set: 2197\n",
      "\n",
      "Class counts for test set:\n",
      "not_base_set    2177\n",
      "base_set          20\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Splitting dataset into a train set, validation set, and test set\n",
    "\n",
    "test_proportion = 1/5\n",
    "val_proportion = 1/5\n",
    "\n",
    "# reading csv into dataframe and transforming filenames into file paths\n",
    "df = pd.read_csv(data_csv_path)\n",
    "df['filename'] = df['filename'].map(lambda f: os.path.join(images_path, f))\n",
    "\n",
    "# Splitting dataset into train, validation, and test datasets\n",
    "df_train, df_val, df_test = data.train_val_test_split(df, test_proportion, val_proportion, seed=random_seed)\n",
    "\n",
    "# Calculating and printing some useful values about the train, validation, and test datasets\n",
    "print_dataset_info(df_train, 'training set')\n",
    "print()\n",
    "print()\n",
    "print_dataset_info(df_val, 'validation set')\n",
    "print()\n",
    "print()\n",
    "print_dataset_info(df_test, 'test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upsampling and downsampling datasets\n",
    "\n",
    "class_samples_train = {'base_set': 100, 'not_base_set': 100}\n",
    "class_samples_val = {'base_set': 40, 'not_base_set': 40}\n",
    "class_samples_test = None\n",
    "\n",
    "replacement_train = {'base_set': True, 'not_base_set': False}\n",
    "replacement_val = {'base_set': True, 'not_base_set': False}\n",
    "replacement_test = None\n",
    "\n",
    "df_train = data.sample_dataset(df_train, samples_per_class=class_samples_train, replacement=replacement_train, \n",
    "                               seed=random_seed)\n",
    "df_val = data.sample_dataset(df_val, samples_per_class=class_samples_val, replacement=replacement_val, seed=random_seed)\n",
    "df_test = data.sample_dataset(df_test, samples_per_class=class_samples_test, replacement=replacement_test,\n",
    "                              seed=random_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training set: 200\n",
      "\n",
      "Class counts for training set:\n",
      "not_base_set    100\n",
      "base_set        100\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "Size of validation set: 80\n",
      "\n",
      "Class counts for validation set:\n",
      "not_base_set    40\n",
      "base_set        40\n",
      "Name: class, dtype: int64\n",
      "\n",
      "\n",
      "Size of test set: 2197\n",
      "\n",
      "Class counts for test set:\n",
      "not_base_set    2177\n",
      "base_set          20\n",
      "Name: class, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculating and printing some useful values for the sampled datasets\n",
    "print_dataset_info(df_train, 'training set')\n",
    "print()\n",
    "print()\n",
    "print_dataset_info(df_val, 'validation set')\n",
    "print()\n",
    "print()\n",
    "print_dataset_info(df_test, 'test set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping dataframe classes to their binary classification encodings\n",
    "\"\"\"\n",
    "This is necessary because tf.keras.preprocessing.image.ImageDataGenerator's flow_from_dataframe doesn't work, \n",
    "specifically the classes arguement does not function.\n",
    "\"\"\"\n",
    "\n",
    "mapping = {'not_base_set': '0', 'base_set': '1'}\n",
    "\n",
    "df_train['class'] = df_train['class'].map(lambda foo: mapping[foo])\n",
    "df_val['class'] = df_val['class'].map(lambda foo: mapping[foo])\n",
    "df_test['class'] = df_test['class'].map(lambda foo: mapping[foo])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 validated image filenames belonging to 2 classes.\n",
      "Found 80 validated image filenames belonging to 2 classes.\n",
      "Found 2197 validated image filenames belonging to 2 classes.\n",
      "{'0': 0, '1': 1}\n",
      "{'0': 0, '1': 1}\n",
      "{'0': 0, '1': 1}\n"
     ]
    }
   ],
   "source": [
    "# Augmenting datasets \n",
    "train_datagen = ImageDataGenerator(height_shift_range=0.5, rotation_range=90, horizontal_flip=True, vertical_flip=True)\n",
    "valid_datagen = ImageDataGenerator(height_shift_range=0.5, rotation_range=90, horizontal_flip=True, vertical_flip=True)\n",
    "test_datagen = ImageDataGenerator(height_shift_range=0.5, rotation_range=90, horizontal_flip=True, vertical_flip=True)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(df_train, target_size=img_size, class_mode='binary',\n",
    "                                                    batch_size=batch_size, drop_duplicates=False, seed=random_seed)\n",
    "valid_generator = valid_datagen.flow_from_dataframe(df_val, target_size=img_size, class_mode='binary',\n",
    "                                                    batch_size=batch_size, drop_duplicates=False, seed=random_seed)\n",
    "test_generator = test_datagen.flow_from_dataframe(df_test, target_size=img_size, class_mode='binary',\n",
    "                                                  batch_size=batch_size, drop_duplicates=False, seed=random_seed)\n",
    "\n",
    "print(train_generator.class_indices)\n",
    "print(valid_generator.class_indices)\n",
    "print(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_1(): \n",
    "    inputs = k.Input(shape=(*img_size, 3))\n",
    "    x = k.layers.Conv2D(5, (2,2), strides=(1,1), padding='same', \n",
    "                      input_shape=(*img_size, 3), activation='relu')(inputs)\n",
    "    x = k.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid')(x)\n",
    "    x = k.layers.Conv2D(10, (5,5), strides=(1,1), padding='same', activation='relu')(x)\n",
    "    x = k.layers.MaxPooling2D(pool_size=(5,5), strides=(5,5), padding='valid')(x)\n",
    "    x = k.layers.Conv2D(25, (10,10), strides=(1,1), padding='same', activation='relu')(x)\n",
    "    x = k.layers.MaxPooling2D(pool_size=(10,10), strides=(10,10), padding='valid')(x)\n",
    "    x = k.layers.Flatten()(x)\n",
    "    x = k.layers.Dense(100, activation='relu')(x)\n",
    "    x = k.layers.Dense(40, activation='relu')(x)\n",
    "    x = k.layers.Dense(15, activation='relu')(x)\n",
    "    x = k.layers.Dense(5, activation='relu')(x)\n",
    "    predictions = k.layers.Dense(1, activation='sigmoid', use_bias=False)(x)\n",
    "    model = k.models.Model(inputs=inputs, outputs=predictions)\n",
    "    model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001), loss=k.losses.BinaryCrossentropy(),\n",
    "                  metrics=[k.metrics.BinaryAccuracy(), k.metrics.Precision(), k.metrics.Recall()])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and Cross-validating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 550, 400, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 550, 400, 5)       65        \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 275, 200, 5)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 275, 200, 10)      1260      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 55, 40, 10)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 55, 40, 25)        25025     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 4, 25)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 100)               50100     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 40)                4040      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 15)                615       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 5)                 80        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81,190\n",
      "Trainable params: 81,190\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = model_1()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "  46/2000 [..............................] - ETA: 34:29 - loss: 1.0984 - binary_accuracy: 0.4970 - precision: 0.5294 - recall: 0.0269"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-650b11123227>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m history = model.fit_generator(train_generator, epochs=10, steps_per_epoch=2000, validation_data=valid_generator,\n\u001b[0;32m----> 2\u001b[0;31m                               shuffle=True, validation_steps=100)\n\u001b[0m",
      "\u001b[0;32m/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[0;32m/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtarget_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m       \u001b[0mbatch_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mbatch_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36m_get_next_batch\u001b[0;34m(generator, mode)\u001b[0m\n\u001b[1;32m    360\u001b[0m   \u001b[0;34m\"\"\"Retrieves the next batch of input data.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m     \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/Data/Programs/Pkmn_card_classifier/env/lib/python3.6/site-packages/tensorflow/python/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=10, steps_per_epoch=2000, validation_data=valid_generator,\n",
    "                              shuffle=True, validation_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating Model\n",
    "scores = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model\n",
    "\n",
    "k.experimental.export_saved_model(model, 'models')\n",
    "\n",
    "# Recreate the exact same model\n",
    "new_model = k.experimental.load_from_saved_model('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bullshit\n",
    "\n",
    "def compresssed_dataset(dataset_path):\n",
    "    classes = os.listdir(dataset_path)\n",
    "    ds = []\n",
    "    for c in classes:\n",
    "        class_path = dataset_path + '/' + c\n",
    "        imgs_of_c = os.listdir(class_path)\n",
    "        ext = lambda img: class_path + '/' + img\n",
    "        imgpaths_of_c = list(map(ext, imgs_of_c))\n",
    "        samples_of_c = list(zip(imgpaths_of_c, [c for _ in range(len(imgpaths_of_c))]))\n",
    "        ds.extend(samples_of_c)\n",
    "    ds = np.asarray(ds)        \n",
    "    return ds\n",
    "        \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "def stratified_partitions(k, dataset_path, seed=None):\n",
    "    random.seed(seed)\n",
    "    classes = os.listdir(dataset_path)\n",
    "    samples_of_classes = []\n",
    "    for c in classes:\n",
    "        samples_of_c = os.listdir(dataset_path + '/' + c)\n",
    "        samples_of_c = list(zip(samples_of_c, [c for _ in range(len(samples_of_c))]))\n",
    "        random.shuffle(samples_of_c)\n",
    "        samples_of_classes.append(samples_of_c)\n",
    "    if (min(list(map(len, samples_of_classes))) / k) < 1:\n",
    "        raise Exception(\"cannot partition datset into {0} partitions and still keep each partition startified\".format(k))\n",
    "    partitions_of_classes = [[samples_of_class[i * (len(samples_of_class) // k) + min(i, len(samples_of_class) % k):(i+1) * (len(samples_of_class) // k) +\n",
    "                                               min(i+1, len(samples_of_class) % k)] for i in range(k)] for samples_of_class in samples_of_classes]\n",
    "    partitions = []\n",
    "    num_classes = len(partitions_of_classes)\n",
    "    for i in range(k):\n",
    "        partition = []\n",
    "        for c in range(num_classes):\n",
    "            partition.extend(partitions_of_classes[c][i])\n",
    "        random.shuffle(partition)\n",
    "        partitions.append(partition)\n",
    "    return partitions\n",
    "\n",
    "\"\"\"\n",
    "samples_per_class: dictionary, with keys that are the names of the different classes, and values that are the number of samples to draw from a specified \n",
    "                   class.\n",
    "dataset_path: string or numpy array, a path to dataset with subfolders containing samples of a specific class with the name of the class as the \n",
    "              subfolde, or a numpy array where each row is a sample and the first column is filename and the second is label\n",
    "replacement: dictionary, with keys that are the names of the different classes, and values that are boolean which specify whether sampling from the\n",
    "             specified class should be done with replacement (True) or not (False)\n",
    "\n",
    "returns: numpy array, of tuples, containing (path to image, label) representing the sampled dataset from the original dataset.\n",
    "\"\"\"\n",
    "def sample_dataset(samples_per_class, dataset, replacement, seed=None):\n",
    "    random.seed(seed)\n",
    "    sampled_set = []\n",
    "    classes = np.unique(dataset[:,1])\n",
    "    if samples_per_class == None:\n",
    "        samples_per_class = {c: np.sum(dataset[:,1] == c) for c in classes}\n",
    "    if replacement == None:\n",
    "        replacement = {c: False for c in classes}\n",
    "    for c in classes:\n",
    "        imgpaths_per_c = dataset[dataset[:,1] == c]\n",
    "        idx = np.random.choice(len(imgpaths_per_c), size=samples_per_class[c], replace=replacement[c])\n",
    "        sampled_set.extend(imgpaths_per_c[idx])       \n",
    "    return np.asarray(sampled_set)  \n",
    "\n",
    "def encode(label):\n",
    "    encoding = {'base_set': '1', 'not_base_set': '0'}\n",
    "    return encoding[label]\n",
    "\n",
    "def encode_dataframe(df):\n",
    "    df['class'] = df['class'].map(encode)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
